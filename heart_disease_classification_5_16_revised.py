# -*- coding: utf-8 -*-
"""Heart disease Classification 5/16 revised

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qYl5-4JSDsDXzoSUXnVmpXGhDd3xHBCM
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from imblearn.combine import SMOTEENN
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import accuracy_score , confusion_matrix , classification_report, mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

from google.colab import drive
drive.mount('/content/drive')

heart = pd.read_csv('/content/drive/MyDrive/heart_2022_with_nans.csv')
heart.head()

heart.info()

len(heart)

"""### missing values"""

missing_values = heart.isnull().sum()

# Convert the missing values to a DataFrame for plotting
missing_values_heart = missing_values.reset_index()
missing_values_heart.columns = ['Column', 'MissingValues']

# Plot the missing values
plt.figure(figsize=(12, 9))
sns.barplot(data=missing_values_heart, y='Column', x='MissingValues', orient='h')
plt.xlabel('Count of Missing Values')
plt.ylabel('Columns')
plt.title('Count of Missing Values in Each Column of the Heart DataFrame')
plt.xlim(0, 445000)  # Set the x-axis scale from 0 to 445000

# Add text labels above each bar
for index, value in enumerate(missing_values_heart['MissingValues']):
    plt.text(value, index, f'{value}', va='top', fontsize=8)

plt.tight_layout()
plt.show()

"""對於數值型的欄位（像是 PhysicalHealthDays、SleepHours 這些），是用 IterativeImputer 搭配 BayesianRidge 模型去預測缺失的數值。

至於類別欄位和布林欄位（像是 Sex、State、HadHeartAttack 等），是用 impute_object_column() 函數，用 RandomForestClassifier 模型根據其他欄位去預測缺失的值，補完後再轉回原本的格式。

### deal with missing values

Imputer only works with the floating data types:
"""

Num_cols = heart.select_dtypes(exclude='object').columns.tolist()
Num_cols

imputer2 = IterativeImputer(max_iter=10, random_state=42)

heart['PhysicalHealthDays'] = imputer2.fit_transform(heart[['PhysicalHealthDays']])
heart['MentalHealthDays']= imputer2.fit_transform(heart[['MentalHealthDays']])
heart['SleepHours'] = imputer2.fit_transform(heart[['SleepHours']])
heart['HeightInMeters'] = imputer2.fit_transform(heart[['HeightInMeters']])
heart['WeightInKilograms'] = imputer2.fit_transform(heart[['WeightInKilograms']])
heart['BMI'] = imputer2.fit_transform(heart[['BMI']])

(heart.isnull().sum()/ len(heart)* 100).sort_values(ascending=False)

"""Handle the column having object data type"""

heart.isnull().sum()[heart.isnull().sum()>0].sort_values(ascending=False)

missing_data_cols = heart.isnull().sum()[heart.isnull().sum()>0].index.tolist()

categorical_cols = ['State', 'Sex', 'AgeCategory', 'RaceEthnicityCategory', 'GeneralHealth','RemovedTeeth', 'TetanusLast10Tdap', 'LastCheckupTime', 'SmokerStatus', 'ECigaretteUsage']
bool_cols = ['HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease','HadArthritis', 'HadDiabetes','DeafOrHardOfHearing', 'BlindOrVisionDifficulty', 'DifficultyConcentrating', 'DifficultyWalking', 'DifficultyDressingBathing', 'DifficultyErrands', 'AlcoholDrinkers', 'PhysicalActivities', 'CovidPos', 'ChestScan', 'HIVTesting', 'HighRiskLastYear', 'FluVaxLast12', 'PneumoVaxEver']
numerical_cols = ['PhysicalHealthDays', 'MentalHealthDays', 'SleepHours', 'HeightInMeters', 'WeightInKilograms', 'BMI']

def impute_object_column(heart, col, missing_data_cols):
    df_null = heart[heart[col].isnull()]
    df_not_null = heart[heart[col].notnull()]

    # Split X and y
    X = df_not_null.drop(col, axis=1)
    y = df_not_null[col]

    # Encode all categorical features in X
    label_encoder_X = LabelEncoder()
    for c in X.columns:
        if X[c].dtype == 'object' or X[c].dtype == 'category' or X[c].dtype == 'bool':
            X[c] = label_encoder_X.fit_transform(X[c].astype(str))

    # Encode y
    label_encoder_y = LabelEncoder()
    y_encoded = label_encoder_y.fit_transform(y.astype(str))

    # Handle other missing values in X using IterativeImputer
    imputer = IterativeImputer(estimator=RandomForestClassifier(), max_iter=5, random_state=42)
    for c in [x for x in missing_data_cols if x != col and X[x].isnull().sum() > 0]:
        X[c] = imputer.fit_transform(X[[c]])

    # Train a classifier
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train, y_train)
    acc = accuracy_score(y_test, clf.predict(X_test))
    print(f"{col} filled with accuracy: {round(acc * 100, 2)}%")

    # Prepare null rows for prediction
    X_null = df_null.drop(col, axis=1)
    for c in X_null.columns:
        if X_null[c].dtype == 'object' or X_null[c].dtype == 'category' or X_null[c].dtype == 'bool':
            X_null[c] = label_encoder_X.fit_transform(X_null[c].astype(str))
    for c in [x for x in missing_data_cols if x != col and X_null[x].isnull().sum() > 0]:
        X_null[c] = imputer.fit_transform(X_null[[c]])

    # Predict missing values and decode
    y_pred_encoded = clf.predict(X_null)
    y_pred = label_encoder_y.inverse_transform(y_pred_encoded)

    # Restore back into main DataFrame
    heart.loc[heart[col].isnull(), col] = y_pred
    return heart[col]

for col in categorical_cols:
    if heart[col].isnull().sum() > 0:
        heart[col] = impute_object_column(heart, col, missing_data_cols)

for col in bool_cols:
    if heart[col].isnull().sum() > 0:
        heart[col] = impute_object_column(heart, col, missing_data_cols)

heart.isnull().sum().sort_values(ascending=False)

len(heart)

"""### remove duplicated data"""

duplicated_rows = heart[heart.duplicated()]
len(duplicated_rows)

heart.drop_duplicates(inplace=True)

"""### EDA"""

heart_count = heart['HadHeartAttack'].value_counts()

# Plot the data
sns.barplot(x=heart_count.index, y=heart_count.values)
plt.xlabel('Heart Disease')
plt.ylabel('Count')
plt.title('Count of Individuals with by Heart Disease')
plt.show()

plt.figure(figsize=(6, 4))
plt.title('Distribution of Males and Females')
sns.set(style='whitegrid')
sns.countplot(x='Sex', data=heart, palette='pastel', hue='Sex', dodge=False, legend=False)
plt.xlabel('Sex')
plt.ylabel('Count')
plt.show()

sex_heart_count = heart['Sex'][heart['HadHeartAttack'] == 'Yes'].value_counts()

# Plot the data
sns.barplot(x=sex_heart_count.index, y=sex_heart_count.values)
plt.xlabel('Sex')
plt.ylabel('Count')
plt.title('Count of Individuals with Heart Disease by Sex')
plt.show()

gh_heart_count = heart['GeneralHealth'].value_counts()

# Plot the data
plt.figure(figsize=(6, 5))
ax = sns.barplot(x=gh_heart_count.index, y=gh_heart_count.values)

# Annotate the bars with the count values
for i, v in enumerate(gh_heart_count.values):
    ax.text(i, v + 1, str(v), ha='center', va='bottom')

plt.xlabel('General Health')
plt.ylabel('Count')
plt.title('Count of Individuals by General Health')
plt.show()

g_gh = heart.groupby('GeneralHealth')

fig, axes = plt.subplots(1, len(g_gh.groups), figsize=(20, 6))

# Plot each group
for i, (g, group) in enumerate(g_gh):
    x_counts = group['Sex'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('Sex')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, len(g_gh.groups), figsize=(20, 6))

# Plot each group
for i, (g, group) in enumerate(g_gh):
    x_counts = group['Sex'][group['HadHeartAttack'] == 'Yes'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('Sex')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

value_counts = heart['SmokerStatus'].value_counts()

plt.figure(figsize=(6, 4))
value_counts.plot(kind='barh', color=['blue', 'orange'])
plt.title('Distribution of Smoker Status')
plt.ylabel('Smoker Status')
plt.xlabel('Count')
plt.xticks(rotation=0)
plt.show()

fig, axes = plt.subplots(1, len(g_gh.groups), figsize=(20, 6))

# Plot each group
for i, (g, group) in enumerate(g_gh):
    x_counts = group['Sex'][(group['HadHeartAttack'] == 'Yes') & (group['SmokerStatus'] == 'Current smoker - now smokes some days') | (group['SmokerStatus'] == 'Current smoker - now smokes every day')].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('Sex')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

def bmi_category(bmi):
    if bmi > 24.9:
        return 'Above Normal'
    elif bmi >= 18.5 and bmi <= 24.9:
        return 'Normal'
    else:
        return 'Below Normal'

# Apply the function to create a new column 'bmi_category'
heart['bmi_category'] = heart['BMI'].apply(bmi_category)

# Create subplots for BMI categories
fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['bmi_category'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('BMI Category')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')


plt.tight_layout()
plt.show()

# Remove the 'bmi_category' column to revert to the original state
heart.drop(columns='bmi_category', inplace=True)

fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x = group['BMI']

    # Plot the data
    sns.histplot(x, kde=True, ax=axes[i])
    axes[i].axvline(x.mean(), color='r', linestyle='--', label=f'Mean: {x.mean():.2f}')
    axes[i].axvline(x.max(), color='g', linestyle='-', label=f'Max: {x.max():.2f}')
    axes[i].axvline(x.min(), color='b', linestyle='-', label=f'Min: {x.min():.2f}')
    axes[i].set_xlabel('BMI')
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'{g}')
    axes[i].legend()

plt.tight_layout()
plt.show()

heart['bmi_category'] = heart['BMI'].apply(bmi_category)

# Create subplots for BMI categories
fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['bmi_category'][group['HadHeartAttack'] == 'Yes'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('BMI Category with Heart Disease')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')


plt.tight_layout()
plt.show()

# Remove the 'bmi_category' column to revert to the original state
heart.drop(columns='bmi_category', inplace=True)

fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x = group['BMI'][group['HadHeartAttack'] == 'Yes']

    # Plot the data
    sns.histplot(x, kde=True, ax=axes[i])
    axes[i].axvline(x.mean(), color='r', linestyle='--', label=f'Mean: {x.mean():.2f}')
    axes[i].axvline(x.max(), color='g', linestyle='-', label=f'Max: {x.max():.2f}')
    axes[i].axvline(x.min(), color='b', linestyle='-', label=f'Min: {x.min():.2f}')
    axes[i].set_xlabel('BMI with Heart Disease')
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'{g}')
    axes[i].legend()

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['AgeCategory'].value_counts()

    # Plot the data
    sns.barplot(y=x_counts.index, x=x_counts.values, ax=axes[i] , orient='h')
    axes[i].set_ylabel('Age')
    axes[i].set_xlabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['AgeCategory'][group['HadHeartAttack'] == 'Yes'].value_counts()

    # Plot the data
    sns.barplot(y=x_counts.index, x=x_counts.values, ax=axes[i] , orient='h')
    axes[i].set_xlabel('Age Category with Heart Disease')
    axes[i].set_ylabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x_counts = group['AlcoholDrinkers'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_ylabel('Alcohol Drinking')
    axes[i].set_xlabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x_counts = group['AlcoholDrinkers'][heart['HadHeartAttack'] == 'Yes'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_ylabel('Alcohol Drinking And Heart Disease')
    axes[i].set_xlabel('Count')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['PhysicalActivities'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('Count')
    axes[i].set_ylabel('Phisical Activity')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(22, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):
    x_counts = group['PhysicalActivities'][group['HadHeartAttack'] == 'Yes'].value_counts()

    # Plot the data
    sns.barplot(x=x_counts.index, y=x_counts.values, ax=axes[i])
    axes[i].set_xlabel('Count')
    axes[i].set_ylabel('Phisical Activity with Heart Disease')
    axes[i].set_title(f'{g}')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x = group['SleepHours']

    # Plot the data
    sns.histplot(x, kde=True, ax=axes[i])
    axes[i].axvline(x.mean(), color='r', linestyle='--', label=f'Mean: {x.mean():.2f}')
    axes[i].axvline(x.max(), color='g', linestyle='-', label=f'Max: {x.max():.2f}')
    axes[i].axvline(x.min(), color='b', linestyle='-', label=f'Min: {x.min():.2f}')
    axes[i].set_xlabel('Sleep Time')
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'{g}')
    axes[i].legend()

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 5, figsize=(18, 6))  # Adjusted figure size

for i, (g, group) in enumerate(g_gh):

    x = group['SleepHours'][group['HadHeartAttack'] == 'Yes']

    # Plot the data
    sns.histplot(x, kde=True, ax=axes[i])
    axes[i].axvline(x.mean(), color='r', linestyle='--', label=f'Mean: {x.mean():.2f}')
    axes[i].axvline(x.max(), color='g', linestyle='-', label=f'Max: {x.max():.2f}')
    axes[i].axvline(x.min(), color='b', linestyle='-', label=f'Min: {x.min():.2f}')
    axes[i].set_xlabel('Sleep Time with Heart Disease')
    axes[i].set_ylabel('Frequency')
    axes[i].set_title(f'{g}')
    axes[i].legend()

plt.tight_layout()
plt.show()

"""### deal with outliers

只處理了 BMI、HeightInMeters、WeightInKilograms 這三個欄位的離群值，像是超過 200 公斤或 2.4 公尺等的資料，用 IQR 方法把它們截斷在合理範圍內。（超過的值設定為Q3 + 1.5 * IQR或Q1 - 1.5 * IQR）

其他像 PhysicalHealthDays、MentalHealthDays 是問卷設計最多就是 30 天，；SleepHours 原本就在 0～24 小時之間，數字再高或再低也可能是真實狀況，所以這三個就沒有動。
"""

plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_cols):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x=heart[col])
    plt.title(col)
plt.tight_layout()
plt.show()

heart.describe()

numericalData = ['HeightInMeters', 'WeightInKilograms', 'BMI']
for i in numericalData:
    Q1 = heart[i].quantile(0.25)
    Q3 = heart[i].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    heart.loc[heart[i] < lower_bound, i] = lower_bound
    heart.loc[heart[i] > upper_bound, i] = upper_bound

heart.describe()

plt.figure(figsize=(12, 8))
for i, col in enumerate(numerical_cols):
    plt.subplot(2, 3, i+1)
    sns.boxplot(x=heart[col])
    plt.title(col)
plt.tight_layout()
plt.show()

"""### Categorical columns"""

for col in heart.select_dtypes(include='object').columns:
    encoder = LabelEncoder()
    heart[col] = encoder.fit_transform(heart[col])

"""### Feature Extraction

如果用這種切箱的方式可以考慮前面的 outliner 是否需要把後面的都截斷！  

即有機會保留老師之前提到的稀缺重要資料（可以看哪種模型效果較好）
"""

def transform_bmi(value):
    if value<16:
        return 0
    elif value>=16 and value<17:
        return 1
    elif value>=17 and value<18.5:
        return 2
    elif value>=18.5 and value<25:
        return 3
    elif value>=25 and value<30:
        return 4
    elif value>=30 and value<35:
        return 5
    elif value>=35 and value<40:
        return 6
    elif value>=40 :
        return 7

heart["BMI"] = heart["BMI"].apply(transform_bmi)
heart["BMI"].value_counts(ascending= False)

"""### Feature Selection"""

X = heart.drop(columns=['HadHeartAttack'])
y = heart['HadHeartAttack']

X.head()

"""先設置使用 iv > 0.02 作為標準，後續可調整是否要稍微放寬至 0.01"""

from sklearn.feature_selection import mutual_info_classif

def calculate_iv(X, y):
    iv_values = mutual_info_classif(X, y)
    iv_df = pd.DataFrame({'Feature': X.columns, 'IV': iv_values})
    return iv_df

iv_df = calculate_iv(pd.DataFrame(X, columns=X.columns), y)
selected_features_iv = iv_df[iv_df['IV'] > 0.01]['Feature'].tolist()

X_iv = X.loc[:, selected_features_iv]

X_iv.columns

"""用隨機森林篩除特徵重要性極低 < 0.01 之特徵變數"""

# 隨機森林取特徵重要性
rf = RandomForestClassifier(random_state=42)
rf.fit(X_iv, y)
feature_importances = pd.DataFrame({'Feature': selected_features_iv, 'Importance': rf.feature_importances_})

importance_threshold = 0.01
top_features_rf = feature_importances[feature_importances['Importance'] > importance_threshold]['Feature'].tolist()
print(f"特徵重要性大於{importance_threshold}的特徵數量：{len(top_features_rf)}")

X_rf = X_iv.loc[:, top_features_rf]
print("隨機森林篩選後特徵數量：", len(top_features_rf))

"""LDA 降維處理（也可視後面模型成效決定是否降維）"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA()
X_lda = lda.fit_transform(X_rf, y)

print("LDA降維後特徵數量：", X_lda.shape[1])

"""### data"""

# 加上stratified sampling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

"""### Normalization"""

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### deal with imbalamced

"""

# 使用 Resampling
smote_enn = SMOTEENN(random_state=42)
X_train, y_train = smote_enn.fit_resample(X_train, y_train)

# 看目前的類別分布是否符合預期
from collections import Counter
print(Counter(y_train))

"""### model"""

models = {
    'KNN': KNeighborsClassifier(),
    'RandomForest': RandomForestClassifier(),
    'DecisionTree': DecisionTreeClassifier()
    ,
    'LogisticRegression': LogisticRegression()
    ,
    'SVC': SVC()
}

param_grids = {
    'KNN': {
        'n_neighbors': [3, 5, 7],
        'weights': ['uniform', 'distance']
    },
    'RandomForest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    },
     'DecisionTree': {
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    ,
    'LogisticRegression': {
        'C': [0.01, 0.1, 1, 10],
        'solver': ['liblinear', 'saga']
    }
    ,
    'SVC': {
        'C': [0.01, 0.1, 1, 10],
        'kernel': ['linear', 'rbf', 'poly']
    }
}

best_models = {}
best_scores = {}

for model_name, model in models.items():
    print(f"Optimizing {model_name}...")
    param_grid = param_grids[model_name]
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_models[model_name] = grid_search.best_estimator_
    best_scores[model_name] = grid_search.best_score_

    print(f"Best Parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best Score for {model_name}: {grid_search.best_score_}")

    y_pred = grid_search.best_estimator_.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    print(f"Test Accuracy for {model_name}: {test_accuracy}\n")

    # Classification Report
    print(f"Classification Report for {model_name}:\n")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    print(f"Confusion Matrix for {model_name}:\n{conf_matrix}")

    # Plot Confusion Matrix
    plt.figure(figsize=(8, 6))
    class_labels = sorted(set(y_test))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

